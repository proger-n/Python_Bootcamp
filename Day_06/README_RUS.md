# Day 02 - Python Bootcamp
*`Topics:`* **Domain-Driven Design, Decorator**

## Exercise 00: Old Style

-----

Вам нужно написать скрипт cache_wiki.py, основной задачей которого будет загрузка страниц из Википедии, но интересующие нас данные - это ссылки в тексте и разделах "См. также", ведущие на другие страницы в самой Википедии. Это означает, что вам не нужно загружать содержимое, а только сохранить представление графа в виде JSON-файла wiki.json, так что вершины хранят страницы, а направленные ребра - ссылки.

Вы можете выбрать любую статью Википедии в качестве стартовой позиции по умолчанию. Кроме того, ваш код должен уметь принимать в качестве аргумента название существующей статьи, чтобы использовать ее вместо статьи по умолчанию (не обязательно связанной с Гарри Поттером). Таким образом, при запуске программы, например:

~$ python cache_wiki.py -p 'Erdős number'

он должен начать парсинг с этой страницы. Обратите внимание на специальную кодировку символов в URL.

Цель состоит в том, чтобы следующие ссылки (только те, которые ведут на другие страницы Википедии, а не во внешний интернет) уходили как минимум на три страницы вглубь каждой ссылки. Этот параметр должен настраиваться с помощью параметра -d, поэтому по умолчанию значение будет равно 3. Но если результат слишком велик (>1000 страниц), ваш код должен прекратить обработку ссылок. Если же он слишком мал (<20 страниц), то выберите какую-нибудь другую начальную страницу по умолчанию. Не забывайте следить за ссылками, ведущими на страницы, которые вы уже посетили. Если страница A ссылается на страницу B, а страница B ссылается на страницу A - это два ребра направленного графа, а не одно.

Каждая страница, которую посещает ваш код, должна записываться в stdout с помощью модуля logging Python с уровнем лога 'INFO'.


Строгих требований к формату JSON-файла, создаваемого вашим кодом, нет, но имейте в виду, что вам придется работать с этим файлом в следующих упражнениях, поэтому вы можете рассмотреть возможность использования существующих библиотек Python для обработки графов, которые поддерживают чтение/запись JSON-файлов.

Чтобы получить дополнительный балл за это упражнение, ваш код может также поддерживать хранение графа в базе данных Neo4J.


## Exercise 01: Shortcuts

-----

Теперь нужно написать программу shortest_path.py, которая должна будет найти длину кратчайшего пути между двумя страницами в вашей сериализованной базе данных (если такие страницы есть):
```
~$ python shortest_path.py --from 'Welsh Corgi' --to 'Solomon'
3
~$ python shortest_path.py --from 'Solomon' --to 'Welsh Corgi' --non-directed
3
```
Обратите внимание на флаг --non-directed. Он означает, что мы рассматриваем все связи как "ненаправленные" или "двунаправленные", поэтому каждое ребро рассматривается одинаково в обоих направлениях. В этом случае между любыми двумя узлами в сериализованном графе существует путь.

По умолчанию (если не указано --non-directed) мы следуем только за направленными ребрами графа. Это означает, что не все страницы в базе данных могут быть доступны с других страниц, особенно если они имеют небольшое количество входящих ссылок. Если путь не существует, ваш скрипт должен вывести 'Path not found'.

Местоположение вики-файла должно быть прочитано из переменной окружения WIKI_FILE. Если файл базы данных не найден, код должен вывести 'Database not found'.

Кроме того, добавьте флаг -v, который включит запись в журнал найденного пути, как показано ниже:
```
~$ python shortest_path.py -v --from 'Welsh Corgi' --to 'Solomon'
'Вельш-корги' -> 'Дрессировка собак' -> 'Кольцо царя Соломона (книга)' -> 'Соломон'
3
```
В этом упражнении вы не должны использовать существующую реализацию алгоритма "кратчайшего пути", предоставляемую существующими библиотеками. Пожалуйста, напишите его самостоятельно.


## Exercise 02: Greatest Magicians

-----

Следующий скрипт render_graph.py должен визуализировать ваш граф страниц (из файла, сгенерированного в EX00, также считывая его из переменной WIKI_FILE env) в виде PNG-изображения wiki_graph.png, с узлами и ребрами. Вы можете использовать для этого любую стороннюю библиотеку.

Главное правило здесь - размер узла должен соответствовать количеству входящих соединений. Чем больше связей - тем больше узел при рендеринге. Таким образом, "самые большие страницы" в вашем наборе данных будут лучше всего видны.

Вы можете получить дополнительный результат в этой задаче, если ваш скрипт опционально сможет генерировать не только .png-файл, но и страницу wiki_graph.html, которая покажет интерактивную визуализацию того же графа. Для этого можно использовать такие библиотеки, как Altair или Bokeh.

